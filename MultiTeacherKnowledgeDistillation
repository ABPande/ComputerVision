import tensorflow as tf
from tensorflow.keras.models import Model
import numpy as np

def multi_teacher_distillation(teacher_models, student_model, train_data, test_data, temperature=5.0, alpha=0.5, epochs=10, batch_size=32):
    """
    Performs multi-teacher knowledge distillation on a given student model.
    
    Parameters:
    - teacher_models: List of pre-trained Keras teacher models
    - student_model: Smaller Keras model to be trained
    - train_data: Tuple (train_images, train_labels) - NumPy array or TensorFlow dataset
    - test_data: Tuple (test_images, test_labels) - NumPy array or TensorFlow dataset
    - temperature: Softening factor for distillation (default: 5.0)
    - alpha: Weight for balancing true label loss vs. distillation loss (default: 0.5)
    - epochs: Number of training epochs (default: 10)
    - batch_size: Batch size for training (default: 32)
    
    Returns:
    - Fully trained student model
    """
    train_images, train_labels = train_data
    test_images, test_labels = test_data
    
    def get_soft_targets(images):
        """Averages soft targets from multiple teachers."""
        soft_targets = np.mean([tf.nn.softmax(teacher(images) / temperature) for teacher in teacher_models], axis=0)
        return soft_targets
    
    def distillation_loss(y_true, y_pred):
        """Custom loss combining true label loss and multi-teacher soft target loss."""
        soft_targets = get_soft_targets(train_images)
        hard_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
        soft_loss = tf.keras.losses.KLDivergence()(soft_targets, tf.nn.softmax(y_pred / temperature))
        return alpha * hard_loss + (1 - alpha) * soft_loss
    
    student_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=distillation_loss, metrics=['accuracy'])
    student_model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_images, test_labels))
    
    return student_model

# Example usage
if __name__ == "__main__":
    from tensorflow.keras.applications import MobileNetV2, ResNet50, MobileNet
    from tensorflow.keras.datasets import cifar10
    
    # Load CIFAR-10 dataset
    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()
    train_images, test_images = train_images / 255.0, test_images / 255.0
    
    # Define multiple teacher models
    teacher1 = MobileNetV2(weights=None, input_shape=(32, 32, 3), classes=10)
    teacher2 = ResNet50(weights=None, input_shape=(32, 32, 3), classes=10)
    student = MobileNet(weights=None, input_shape=(32, 32, 3), classes=10)
    
    # Perform multi-teacher knowledge distillation
    trained_student = multi_teacher_distillation([teacher1, teacher2], student, (train_images, train_labels), (test_images, test_labels))
    
    # Display student model summary
    trained_student.summary()
