import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV

# Step Decay Learning Rate Scheduler
def step_decay(epoch, initial_lr=0.01, drop=0.5, epochs_drop=10):
    return initial_lr * (drop ** (epoch // epochs_drop))

# Cyclical Learning Rate (CLR)
class CyclicalLearningRate(tf.keras.callbacks.Callback):
    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000.):
        super(CyclicalLearningRate, self).__init__()
        self.base_lr = base_lr
        self.max_lr = max_lr
        self.step_size = step_size
        self.lr_values = []

    def on_train_batch_begin(self, batch, logs=None):
        cycle = np.floor(1 + batch / (2 * self.step_size))
        x = np.abs(batch / self.step_size - 2 * cycle + 1)
        lr = self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x))
        self.model.optimizer.lr.assign(lr)
        self.lr_values.append(lr)

# Polynomial Decay Learning Rate
def polynomial_decay(epoch, initial_lr=0.01, power=1.0, total_epochs=50):
    return initial_lr * (1 - (epoch / total_epochs)) ** power

# Compile Model with Optimizer and Scheduler
def compile_model(model, optimizer='adam', learning_rate=0.001):
    if optimizer == 'adam':
        opt = Adam(learning_rate=learning_rate)
    elif optimizer == 'sgd':
        opt = SGD(learning_rate=learning_rate)
    elif optimizer == 'rmsprop':
        opt = RMSprop(learning_rate=learning_rate)
    else:
        raise ValueError("Unsupported optimizer. Choose from: 'adam', 'sgd', 'rmsprop'")

    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Hyperparameter Tuning with Optimizers & Learning Rate Schedulers
def optimizer_scheduler_tuning(model_fn, train_images, train_labels, param_grid=None, n_iter=10):
    """
    Tunes optimizers and learning rate schedules for a given model.

    Parameters:
    - model_fn: Function returning a compiled Keras model.
    - train_images: NumPy array of training images.
    - train_labels: NumPy array of training labels.
    - param_grid: Dictionary of hyperparameters to tune.
    - n_iter: Number of iterations for RandomizedSearchCV.

    Returns:
    - Best model found.
    - Best hyperparameters.
    """
    if param_grid is None:
        param_grid = {
            'optimizer': ['adam', 'sgd', 'rmsprop'],
            'learning_rate': [0.001, 0.0001, 0.01],
            'scheduler': ['step_decay', 'cyclical', 'polynomial']
        }

    model = KerasClassifier(build_fn=model_fn, verbose=0)

    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=n_iter, cv=3, verbose=1, n_jobs=-1)
    random_search.fit(train_images, train_labels)

    return random_search.best_estimator_, random_search.best_params_

# Example Model Definition
def build_model():
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])
    return model

# Example Usage
if __name__ == "__main__":
    from tensorflow.keras.datasets import cifar10

    (train_images, train_labels), (_, _) = cifar10.load_data()
    train_images = train_images / 255.0  # Normalize images

    # Define Model with Learning Rate Scheduler
    def model_fn():
        model = build_model()
        compiled_model = compile_model(model)
        return compiled_model

    best_model, best_params = optimizer_scheduler_tuning(model_fn, train_images, train_labels)
    print("Best Hyperparameters:", best_params)

    # Training Best Model with Selected Scheduler
    selected_scheduler = best_params['scheduler']
    if selected_scheduler == 'step_decay':
        scheduler_callback = LearningRateScheduler(lambda epoch: step_decay(epoch, initial_lr=best_params['learning_rate']))
    elif selected_scheduler == 'cyclical':
        scheduler_callback = CyclicalLearningRate(base_lr=0.0001, max_lr=0.01, step_size=2000)
    elif selected_scheduler == 'polynomial':
        scheduler_callback = LearningRateScheduler(lambda epoch: polynomial_decay(epoch, initial_lr=best_params['learning_rate'], total_epochs=50))
    else:
        scheduler_callback = None

    if scheduler_callback:
        best_model.model.fit(train_images, train_labels, epochs=20, batch_size=64, callbacks=[scheduler_callback])

    best_model.model.summary()
